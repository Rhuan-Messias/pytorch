{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "10Cid00ScIZGwaJ3rhupmLaWV3UrxJGSg",
      "authorship_tag": "ABX9TyMcXosMzfiE3NH/w1D0lCj3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### imports"
      ],
      "metadata": {
        "id": "NRfSTlH-isBi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfRSbUxpZUgH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Back Propagation"
      ],
      "metadata": {
        "id": "oE1l3rJ-iya1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w1 = torch.tensor(5.0, requires_grad=True)\n",
        "w2 = torch.tensor(10.0, requires_grad=True)\n",
        "\n",
        "r = (x+y)*2\n",
        "\n",
        "for i in range(3):\n",
        "    #forward\n",
        "    z = x*w1 +y*w2\n",
        "    print(z)\n",
        "\n",
        "    loss = (z-r)**2\n",
        "\n",
        "    #back\n",
        "    loss.backward()\n",
        "    print(w1.grad)\n",
        "    print(w2.grad)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      w1 -= 0.1*w1.grad\n",
        "      w2 -= 0.1*w2.grad\n",
        "\n",
        "      w1.grad.zero_()\n",
        "      w2.grad.zero_()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxhcaKc-ZciN",
        "outputId": "e79c7474-c7d4-479a-d497-94f61a5a29f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(25., grad_fn=<AddBackward0>)\n",
            "tensor(38.)\n",
            "tensor(76.)\n",
            "tensor(6., grad_fn=<AddBackward0>)\n",
            "tensor(0.)\n",
            "tensor(0.)\n",
            "tensor(6., grad_fn=<AddBackward0>)\n",
            "tensor(0.)\n",
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(10, device=\"cuda\")\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQp3iu5GrlBh",
        "outputId": "40520a36-5170-4947-d58e-304d9b864fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fazendo o treinamento com numpy"
      ],
      "metadata": {
        "id": "meWcb1M29I29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = np.array([1,2,3,4], dtype=np.float32)\n",
        "Y = np.array([2,4,6,8], dtype=np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "#model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "#loss = MSE\n",
        "def loss(y,y_prediction):\n",
        "  return ((y_prediction - y) ** 2).mean()\n",
        "\n",
        "#gradient\n",
        "# MSE = 1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x - y)\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x, y_predicted - y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "#Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 10\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction = forward_pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #gradients\n",
        "  dw = gradient(X, Y, y_pred)\n",
        "\n",
        "  #update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKBgNOy4jLGn",
        "outputId": "016e655d-a6c8-497f-eb2e-7b6796d208c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 2: w = 1.680, loss = 4.80000067\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 4: w = 1.949, loss = 0.12288000\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "epoch 6: w = 1.992, loss = 0.00314574\n",
            "epoch 7: w = 1.997, loss = 0.00050332\n",
            "epoch 8: w = 1.999, loss = 0.00008053\n",
            "epoch 9: w = 1.999, loss = 0.00001288\n",
            "epoch 10: w = 2.000, loss = 0.00000206\n",
            "Prediction after training: f(5) = 9.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fazendo o treinamento com PyTorch"
      ],
      "metadata": {
        "id": "RPMHtfbF9OVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "#model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "#loss = MSE\n",
        "def loss(y,y_prediction):\n",
        "  return ((y_prediction - y) ** 2).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "#Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction = forward_pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #gradients\n",
        "  l.backward() # dl/dw PyTorch do all the computation of local gradients\n",
        "\n",
        "  #update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  #zero gradients\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNY8NADM3nZd",
        "outputId": "9213efc3-700e-4da2-88e9-d16428b6667b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1 ) Design model (input, output size, forward pass)\n",
        "#2 ) Construct loss and optimizer\n",
        "#3 ) Traning loop\n",
        "#  - forward pass: compute prediction\n",
        "#  - backward pass: gradients\n",
        "#  - update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "#model = nn.Linear(input_size, output_size)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    #define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "#Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction = forward_pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #gradients\n",
        "  l.backward() # dl/dw PyTorch do all the computation of local gradients\n",
        "\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "  #zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78gPImMecGdu",
        "outputId": "a9a0cb3b-e23d-4423-d239-b45349bc3181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Prediction before training: f(5) = 2.948\n",
            "epoch 1: w = 0.925, loss = 16.61024857\n",
            "epoch 11: w = 1.858, loss = 0.43163365\n",
            "epoch 21: w = 2.007, loss = 0.01294493\n",
            "epoch 31: w = 2.030, loss = 0.00200897\n",
            "epoch 41: w = 2.033, loss = 0.00162860\n",
            "epoch 51: w = 2.032, loss = 0.00152699\n",
            "epoch 61: w = 2.031, loss = 0.00143794\n",
            "epoch 71: w = 2.031, loss = 0.00135424\n",
            "epoch 81: w = 2.030, loss = 0.00127540\n",
            "epoch 91: w = 2.029, loss = 0.00120117\n",
            "Prediction before training: f(5) = 10.058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Linear Regression"
      ],
      "metadata": {
        "id": "9nvVnGjIG5OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Design model (input, output size, forward pass)\n",
        "# Construc loss and optimizer\n",
        "# Training Loop\n",
        "#  - forward pass: compute prediction and loss\n",
        "#  - backward pass: gradients\n",
        "#  - update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#preprocess (prepare data)\n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1,\n",
        "                                            noise=20, random_state=1)\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))# they were double before\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "y = y.view(y.shape[0], 1) #reshape our tensor\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# step 1 - model\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# step 2 - loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #Stochastic Gradient Descent\n",
        "\n",
        "# step 3 - traning loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  #forward pass and loss\n",
        "  y_predicted = model(X)\n",
        "  loss = criterion(y_predicted, y) # Predicted and actual values\n",
        "\n",
        "  #backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  #update\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "#plot\n",
        "predicted = model(X).detach() #don't show in the graph (create a new tensor where the req_gradient is false)\n",
        "\n",
        "plt.plot(X_numpy, y_numpy, 'ro') # red dots = ro\n",
        "plt.plot(X_numpy, predicted, 'b') # blue line = b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "7598FfWGHxE5",
        "outputId": "9bbaa089-8d2c-4be4-df93-4e97518ab5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 4439.2251\n",
            "epoch: 20, loss = 3310.5303\n",
            "epoch: 30, loss = 2494.0000\n",
            "epoch: 40, loss = 1902.6608\n",
            "epoch: 50, loss = 1473.9783\n",
            "epoch: 60, loss = 1162.9244\n",
            "epoch: 70, loss = 937.0290\n",
            "epoch: 80, loss = 772.8491\n",
            "epoch: 90, loss = 653.4370\n",
            "epoch: 100, loss = 566.5279\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7e77974a3e90>]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "Oe1uLt73uer6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 0 Prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples,  n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "#scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0], 1) #reshape the tensor, each value in one row\n",
        "y_test = y_test.view(y_test.shape[0],1)\n",
        "\n",
        "# 1 - Model\n",
        "# f = wx + b, sigmoid at the end\n",
        "class LogisticRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "# 2 - Loss and optimizer\n",
        "# loss is different from the linear regression case\n",
        "#Binary Cross Entropy\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3 - training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  #forward pass and loss\n",
        "  y_predicted = model(X_train)\n",
        "  loss = criterion(y_predicted, y_train)\n",
        "\n",
        "  #backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  #updates\n",
        "  optimizer.step()\n",
        "\n",
        "  #zero gradients because backward function add up the old gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if(epoch+1) % 10 ==0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "with torch.no_grad(): #creates the tensor with req_gradient=False\n",
        "  y_predicted = model(X_test)\n",
        "  y_predicted_cls = y_predicted.round()\n",
        "  accuracy = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0]) #every correct it add plus one\n",
        "  print(f'accuracy = {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySda1JlmpUM6",
        "outputId": "ac38c42a-ed56-403b-adc9-a5e9fd369b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 0.4478\n",
            "epoch: 20, loss = 0.3914\n",
            "epoch: 30, loss = 0.3520\n",
            "epoch: 40, loss = 0.3227\n",
            "epoch: 50, loss = 0.2998\n",
            "epoch: 60, loss = 0.2812\n",
            "epoch: 70, loss = 0.2657\n",
            "epoch: 80, loss = 0.2526\n",
            "epoch: 90, loss = 0.2413\n",
            "epoch: 100, loss = 0.2314\n",
            "accuracy = 0.9123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset and DataLoader"
      ],
      "metadata": {
        "id": "k5dOn1dbupdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://raw.githubusercontent.com/patrickloeber/pytorchTutorial/refs/heads/master/data/wine/wine.csv\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "class WineDataset(Dataset): #inherit Dataset\n",
        "  def __init__(self):\n",
        "    #data loading\n",
        "    xy = np.loadtxt('wine.csv', delimiter=',', dtype=np.float32, skiprows=1) #jump the header row\n",
        "    self.x = torch.from_numpy(xy[:, 1:])\n",
        "    self.y = torch.from_numpy(xy[:, [0]]) # n_samples, 1\n",
        "    self.n_samples = xy.shape[0]\n",
        "\n",
        "  def __getitem__(self,index): #  funciona com braskets x[n]\n",
        "    # dataset[0]\n",
        "    return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self): # len() não funcionaria, precisa do __.__\n",
        "    #len(dataset)\n",
        "    return self.n_samples\n",
        "\n",
        "\n",
        "dataset = WineDataset()\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "datatiter = iter(dataloader)\n",
        "data = next(datatiter)\n",
        "features, labels = data\n",
        "\n",
        "#training_loop\n",
        "num_epochs = 2\n",
        "total_samples = len(dataset)\n",
        "n_iterations = math.ceil(total_samples/4)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (inputs, labels) in enumerate(dataloader):\n",
        "    # forward backward, update weights\n",
        "    if (i+1) % 5 ==0:\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, step{i+1}/{n_iterations}, inputs {inputs.shape}')\n"
      ],
      "metadata": {
        "id": "KVpjPUsqutRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b83706b-ff07-435d-a673-26d66c34425b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/2, step5/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step10/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step15/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step20/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step25/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step30/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step35/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step40/45, inputs torch.Size([4, 13])\n",
            "epoch 1/2, step45/45, inputs torch.Size([2, 13])\n",
            "epoch 2/2, step5/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step10/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step15/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step20/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step25/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step30/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step35/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step40/45, inputs torch.Size([4, 13])\n",
            "epoch 2/2, step45/45, inputs torch.Size([2, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Transforms"
      ],
      "metadata": {
        "id": "huE56sy-ZtTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "class WineDataset(Dataset): #inherit Dataset\n",
        "  def __init__(self,  transform=None):\n",
        "    #data loading\n",
        "    xy = np.loadtxt('/content/drive/MyDrive/Colab Notebooks/arquivos/wine.csv', delimiter=',', dtype=np.float32, skiprows=1) #jump the header row\n",
        "    self.n_samples = xy.shape[0]\n",
        "\n",
        "    self.x = xy[:, 1:]\n",
        "    self.y = xy[:, [0]]\n",
        "\n",
        "    self.transform = transform\n",
        "\n",
        "\n",
        "  def __getitem__(self,index): #  funciona com braskets x[n]\n",
        "    sample = self.x[index], self.y[index]\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return sample\n",
        "\n",
        "  def __len__(self): # len() não funcionaria, precisa do __.__\n",
        "    #len(dataset)\n",
        "    return self.n_samples\n",
        "\n",
        "class ToTensor: #custom transform\n",
        "  def __call__(self, sample): #callable object\n",
        "    inputs, targets = sample\n",
        "    return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "class MulTransform:   # multiplication transform\n",
        "  def __init__(self, factor):\n",
        "    self.factor = factor\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    inputs, target = sample\n",
        "    inputs *= self.factor\n",
        "\n",
        "    return inputs, target\n",
        "\n",
        "dataset = WineDataset(transform=ToTensor())\n",
        "firstz_data = dataset[0]\n",
        "features, labels = firstz_data\n",
        "print(features)\n",
        "print(type(features))\n",
        "print(type(labels))\n",
        "\n",
        "\n",
        "#list of our transformationss\n",
        "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])\n",
        "\n",
        "dataset2= WineDataset(transform=composed)\n",
        "first_data = dataset2[0]\n",
        "features,labels = first_data\n",
        "print(features)\n",
        "print(type(features), type(labels))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVOinL9Na_LP",
        "outputId": "14e8f574-5314-4516-e8aa-354ac5e8c0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03])\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "tensor([2.8460e+01, 3.4200e+00, 4.8600e+00, 3.1200e+01, 2.5400e+02, 5.6000e+00,\n",
            "        6.1200e+00, 5.6000e-01, 4.5800e+00, 1.1280e+01, 2.0800e+00, 7.8400e+00,\n",
            "        2.1300e+03])\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax and Cross-Entropy"
      ],
      "metadata": {
        "id": "lod9_5a2gLiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "#creating softmax function\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
        "\n",
        "x = np.array([2.0,1.0,0.1])\n",
        "outputs = softmax(x)\n",
        "print('softmax numpy: ', outputs)\n",
        "\n",
        "x = torch.tensor([2.0,1.0,0.1])\n",
        "outputs = torch.softmax(x, dim=0) #softmax from pytorch\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "id": "zkvgDtvJgOT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63f2923-fe3c-4462-80ca-751e1e80fe47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax numpy:  [0.65900114 0.24243297 0.09856589]\n",
            "tensor([0.6590, 0.2424, 0.0986])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cross-entropy\n",
        "#the better the prediction, the lower the loss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def cross_entropy(actual, predicted):\n",
        "  loss = -np.sum(actual * np.log(predicted))\n",
        "  return loss\n",
        "\n",
        "#y must be one hot encoded\n",
        "# if class 0: [1 0 0]\n",
        "# if class 1: [0 1 0]\n",
        "# if class 2: [0 0 1]\n",
        "\n",
        "Y = np.array([1, 0, 0])\n",
        "\n",
        "# y_pred has the probabilities\n",
        "Y_pred_good = np.array([0.7, 0.2, 0.1]) # after softmax\n",
        "Y_pred_bad = np.array([0.1,0.3,0.6])\n",
        "l1 = cross_entropy(Y, Y_pred_good)\n",
        "l2 = cross_entropy(Y, Y_pred_bad)\n",
        "print(f'Loss 1 numpy: {l1:.4f}')\n",
        "print(f'Loss 2 numpy: {l2:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w1wcV-XMDef",
        "outputId": "bae4fee5-d772-4f9e-c75f-d2da05028f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 1 numpy: 0.3567\n",
            "Loss 2 numpy: 2.3026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross entropy in Pytorch"
      ],
      "metadata": {
        "id": "IVm2u0zrOZ6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "#already do the softmax, dont implement yourself\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3 samples, first is class 2, second 0 and third 1\n",
        "Y = torch.tensor([2,0,1])\n",
        "\n",
        "#size -> n_samples x nclasses = 3x3\n",
        "#raw value, dont apply softmax\n",
        "Y_pred_good = torch.Tensor([[1.0,1.0,2.1],[2.0,1.0,0.1],[0.0,3.0,0.1]])\n",
        "Y_pred_bad = torch.tensor([[0.5,2.0,0.3],[0.2,1.0,0.1],[2.5,1.0,0.1]])\n",
        "\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "\n",
        "print(f'Loss 1 numpy: {l1.item():.4f}')\n",
        "print(f'Loss 2 numpy: {l2.item():.4f}')\n",
        "\n",
        "#to get the actual prediction\n",
        "_, prediction1 = torch.max(Y_pred_good, 1)\n",
        "_, prediction2 = torch.max(Y_pred_bad,1)\n",
        "\n",
        "print(f'pred 1 = {prediction1}')\n",
        "print(f'pred 2 = {prediction2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL2nMg7XOZMg",
        "outputId": "32449b06-9167-4d1c-b842-834aa1468589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 1 numpy: 0.3423\n",
            "Loss 2 numpy: 1.7440\n",
            "pred 1 = tensor([2, 0, 1])\n",
            "pred 2 = tensor([1, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of neural network MultiClass case"
      ],
      "metadata": {
        "id": "rsbnabtsRh-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "\n",
        "#multiclass problem\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNet2, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    # no softmax at the end\n",
        "    return out\n",
        "\n",
        "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
        "criterion = nn.CrossEntropyLoss() #already applies Softmax"
      ],
      "metadata": {
        "id": "GLy7q1zjRl9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary Classification case (yes or no)"
      ],
      "metadata": {
        "id": "DE2MnPSaSs5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Binary Classification\n",
        "class NeuralNet1(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(NeuralNet1, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    #sigmoid at the end\n",
        "    y_pred = torch.sigmoid(out)\n",
        "    return y_pred\n",
        "\n",
        "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "6Y4HKBcaSwJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Functions"
      ],
      "metadata": {
        "id": "fNxhPVpAT5Dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions apply a non-linear transformation and decide whether a neuron should be activated or not:\n",
        "1. step function (0 or 1)\n",
        "2. sigmoid (between 0 and 1)\n",
        "3. TanH (between -1 and 1)\n",
        "4. ReLU (negatives are 0)\n",
        "5. Leaky ReLU (negatives has small value)\n",
        "6. Softmax (give a probabilite that sums up to 1)"
      ],
      "metadata": {
        "id": "jsXnyo_jtP5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a neural network with all the functions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#nn.Sigmoid\n",
        "#nn.Softmax\n",
        "#nn.TanH\n",
        "#nn.LeakyReLU\n",
        "#torch.softmax\n",
        "#torch.tanh . . . etc\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "    self.sigmoid = nn.sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "# option 2 (use activation functions directly in forward pass)\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.relu(self.linear1(x))\n",
        "    out = torch.sigmoid(self.linear2(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "QxsZ0BD7T7uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed-Forward Neural Network"
      ],
      "metadata": {
        "id": "_HlIbv3mxNi7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YLmt3MCMxQN7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}